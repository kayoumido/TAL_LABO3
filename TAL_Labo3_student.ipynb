{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 3\n",
    "# Analyse syntaxique du français\n",
    "\n",
    "**Objectifs**\n",
    "1.\tAppliquer un analyseur syntaxique **de dépendances** sur des données de test en français et observer les scores.  Entraîner cet analyseur sur des données d'entraînement adaptées, et déterminer si les scores se sont améliorés ou non.\n",
    "2.\tImporter des fichiers annotés dans NLTK, les transformer en **graphes de dépendances**, et trouver les paires sujet-verbe les plus fréquentes.\n",
    "3.\tEn appliquant un analyseur **en constituants**, extraire tous les groupes nominaux d'un corpus en français.\n",
    "\n",
    "**Instructions initiales**\n",
    "\n",
    "Vous utiliserez les mêmes données que pour le labo 2, car elles contiennent aussi des informations syntaxiques.  Pour rappel, ces données sont disponibles dans [l'archive ZIP fournie sur Cyberlearn au labo 2](https://cyberlearn.hes-so.ch/pluginfile.php/3493142/mod_assign/introattachment/0/UD_French-GSD-withBlankLines.zip?forcedownload=1).  Ces textes en français proviennent du projet [Universal Dependencies (UD)](https://github.com/UniversalDependencies/UD_French-GSD).  Le fichier `fr-ud-train.conllu3` est destiné à l'entraînement, `fr-ud-dev.conllu3` aux tests/réglages préliminaires, et `fr-ud-test.conllu3` à l'évaluation finale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Appliquer un analyseur de dépendances.  Entraîner l'analyseur sur les données UD\n",
    "\n",
    "Dans cette 1ère partie, vous allez appliquer (puis entraîner) le [Stanford Dependency Parser](https://nlp.stanford.edu/software/nndep.html) qui utilise un réseau de neurones.  Comme au Labo 2, vous aurez besoin du code, d'un modèle entraîné, et de la documentation.\n",
    "\n",
    "* **code Java** : on peut le télécharger dans un [package fourni par Stanford](https://nlp.stanford.edu/software/lex-parser.html) avec d'autres parsers et modèles, mais *on vous recommande* de simplement télécharger l'archive `stanford-corenlp-3.9.2.jar` depuis le [site Maven de Stanford CoreNLP](https://search.maven.org/artifact/edu.stanford.nlp/stanford-corenlp/3.9.2/jar) (choisir *Browse* ou *Downloads*)\n",
    "* **modèles** : plusieurs modèles sont disponibles sur le site Maven, donc un package pour le français de 272 MB ; comme vous n'aurez besoin que du modèle (UD) pour Universal Dependencies, *on vous recommande* de télécharger le modèle pour UD pré-entraîné `UD_French.gz` mis à disposition pour le cours TAL sur [drive.switch.ch](https://drive.switch.ch/index.php/s/OqlSRUCSBvqKg3O)\n",
    "* dans la [documentation du parser](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/nndep/DependencyParser.html) regarder surtout le `main()` et les exemples à la fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail demandé**\n",
    "\n",
    "Utiliser la documentation pour effectuer les tâches suivantes.\n",
    "\n",
    "1. Exécuter le parser en Java (avec une commande externe `!java -cp ...` comme au labo 2) en l'appliquant au fichier UD de *test* en français.  Quels sont les deux scores obtenus et que signifient-ils ?\n",
    "2. Entraîner le parser pour générer un nouveau modèle à partir des données d’entraînement.\n",
    "3. Exécuter le parser que vous avez entraîné sur le même fichier de test qu'en (1).  Quels scores obtient-il et comment cela se compare avec le modèle fourni ?\n",
    "\n",
    "**Note** : il est aussi possible d'appeler ce parser depuis NLTK, à condition de lancer le [serveur CoreNLP](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html) et d'utiliser une instance de la classe `CoreNLPDependencyParser` ainsi : `dep_parse = next(dep_parser.raw_parse(\"La maison est petite.\"))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depparse model: UD_French.gz ... \n",
      "###################\n",
      "#Transitions: 81\n",
      "#Labels: 40\n",
      "ROOTLABEL: root\n",
      "PreComputed 99996, Elapsed Time: 13.509 (s)\n",
      "Initializing dependency parser ... done [15.0 sec].\n",
      "Test File: fr-ud-dev.conllu3\n",
      "OOV Words: 2716 / 35771 = 7.59%\n",
      "UAS = 57.2195\n",
      "LAS = 43.6722\n",
      "DependencyParser parsed 35771 words in 1478 sentences in 19.3s at 1857.7 w/s, 76.8 sent/s.\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici la commande pour tester le parser avec le modèle pré-entraîné.\n",
    "!java -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser \\\n",
    "    -model 'UD_French.gz' \\\n",
    "    -testFile 'fr-ud-dev.conllu3' \\\n",
    "    -verboseResults false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score obtenu: \n",
    "UAS = 57.2195\n",
    "LAS = 43.6722\n",
    "\n",
    "# TODO c'est quoi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indications pour l'entraînement**\n",
    "* pensez donner un nouveau nom au modèle qui sera créé (attention à l'écrasement)\n",
    "* notez que vous utiliserez à la fois `train` *et* `dev` (voir la documentation)\n",
    "* évitez un output trop volumineux dans le notebook : redirigez le tout dans un fichier `output.txt` en faisant suivre la commande par `>output.txt 2>&1` (valable sous Linux et Windows)\n",
    "* plusieurs options indiquées dans la documentation peuvent être utiles\n",
    "  * `-wordCutOff 3` pour traiter seulement les mots apparaissant plus de 3 fois, ce qui évite notamment le problème des nombres écrits avec un espace (apparaissant 1 fois)\n",
    "  * `-trainingThreads 4` pour utiliser pleinement votre processeur : indiquez le maximum selon votre modèle\n",
    "  * `-maxIter 5000` pour arrêter l'entraînement après 5000 itérations (on peut commencer avec beaucoup moins pour estimer le temps nécessaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici la commande pour entraîner l'analyseur sur le fichier 'train' et créer un nouveau modèle.\n",
    "# Pour ne pas bloquer le notebook, vous pouvez l'exécuter en ligne de commande.\n",
    "!java -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser \\\n",
    "    -trainFile fr-ud-train.conllu3 \\\n",
    "    -devFile fr-ud-dev.conllu3 \\\n",
    "    -model cool_model.gz \\\n",
    "    -wordCutOff 3 \\\n",
    "    -maxIter 5000 \\\n",
    "    >output.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depparse model: cool_model.gz ... \n",
      "Loaded TreebankLanguagePack: edu.stanford.nlp.trees.PennTreebankLanguagePack\n",
      "###################\n",
      "#Transitions: 91\n",
      "#Labels: 45\n",
      "ROOTLABEL: root\n",
      "PreComputed 100000, Elapsed Time: 1.53 (s)\n",
      "Initializing dependency parser ... done [2.8 sec].\n",
      "Test File: fr-ud-test.conllu3\n",
      "OOV Words: 1100 / 10020 = 10.98%\n",
      "UAS = 78.0938\n",
      "LAS = 71.3972\n",
      "DependencyParser parsed 10020 words in 416 sentences in 1.0s at 9606.9 w/s, 398.8 sent/s.\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici la commande pour tester l'analyseur avec le nouveau modèle, puis commentez les résultats ci-dessous.\n",
    "!java -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser \\\n",
    "    -model 'cool_model.gz' \\\n",
    "    -testFile 'fr-ud-test.conllu3' \\\n",
    "    -verboseResults false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question subsidiaire**\n",
    "\n",
    "Écrivez des instructions pour extraire du fichier de logs du *DependencyParser*, une fois l'entraînement fini, l'évolution des scores suivants :\n",
    "* le score UAS sur l'ensemble de développement (écrit par défaut tous les 100 itérations)\n",
    "* le taux de correction et la fonction de coût (sur l'ensemble d'entraînement) aux mêmes itérations que les scores UAS\n",
    "\n",
    "Affichez ces trois valeurs sur un graphe, en fonction du nombre d'itérations sur l'axe horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code qui extrait les valeurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrivez ici le code affichant les trois courbes.  \n",
    "# Pensez-vous qu'il soit utile d'entraîner le parser plus longtemps ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tExtraire des données UD les paires sujet-verbe les plus fréquentes\n",
    "\n",
    "Dans cette 2e partie, vous allez d'abord importer les données UD avec l'annotation syntaxique *en dépendances*, grâce à un convertisseur appelé `nltk.parse.DependencyGraph`.  Celui-ci permet de transformer une phrase annotée au format CONLL (c'est-à-dire sur plusieurs lignes avec un mot et ses annotations par ligne) en un graphe de dépendances.  Vous pourrez ensuite faire des statistiques de fréquences sur les trois fichiers `fr-ud-XXX.conllu3`.\n",
    "\n",
    "**Note 1** : on ne peut pas utiliser `ConllCorpusReader` comme dans le labo 2, parce que l'information syntaxique des données UD françaises est faite de *dépendances*, or ConllCorpusReader s'attend à trouver une information de faite de *constituants*.\n",
    "\n",
    "**Note 2** : on utilise ici l'annotation fournie avec le corpus, mais on aurait pu aussi effectuer l'analyse syntaxique avec le parser de Stanford obtenu ci-dessus, pour un texte nouveau.\n",
    "\n",
    "**Travail demandé**\n",
    "* Lisez d'abord les [explications fournies par NLTK](http://www.nltk.org/howto/dependency.html) (début de la page)  pour transformer une phrase au format CONLL en un graphe de dépendances (classe `DependencyGraph`) et accéder aux informations de ce graphe.  Ensuite :\n",
    "* Parcourez le(s) fichier(s) UD phrase par phrase (attention aux espaces dans certains nombres), et créez un graphe de dépendance pour chaque phrase.  Attention, il faut préciser que la relation de plus haut niveau est 'root' (et non 'ROOT').\n",
    "* Sélectionnez les triplets ayant une relation *nsubj* (entre sujet et verbe).\n",
    "* Donnez les 10 triplets les plus fréquents dans tout le corpus (train + dev + test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_conll(file):\n",
    "    phrases = []\n",
    "    with open(file, 'r', encoding='utf8') as fd:\n",
    "        lines = fd.readlines()\n",
    "\n",
    "        phrase = \"\"\n",
    "        for line in lines:\n",
    "            if line == \"\\n\":\n",
    "                phrases.append(phrase)\n",
    "                phrase = \"\"\n",
    "                continue\n",
    "            phrase += line + \"\\n\"\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases:  416\n",
      "400th phrase:  1\tLe\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Sing|PronType=Art\t2\tdet\t_\t_\n",
      "\n",
      "2\tpré\tpré\tNOUN\t_\tGender=Masc|Number=Sing\t11\tnsubj\t_\t_\n",
      "\n",
      "3\tsalé\tsaler\tVERB\t_\tGender=Masc|Number=Sing|Tense=Past|VerbForm=Part\t2\tacl\t_\t_\n",
      "\n",
      "4\tdans\tdans\tADP\t_\t_\t7\tcase\t_\t_\n",
      "\n",
      "5\tson\tson\tDET\t_\tGender=Masc|Number=Sing|Poss=Yes|PronType=Prs\t7\tdet\t_\t_\n",
      "\n",
      "6\tdernier\tdernier\tADJ\t_\tGender=Masc|Number=Sing\t7\tamod\t_\t_\n",
      "\n",
      "7\tméandre\tméandre\tNOUN\t_\tGender=Masc|Number=Sing\t11\tobl\t_\t_\n",
      "\n",
      "8\tavant\tavant\tADP\t_\t_\t10\tcase\t_\t_\n",
      "\n",
      "9\tl'\tle\tDET\t_\tDefinite=Def|Number=Sing|PronType=Art\t10\tdet\t_\tSpaceAfter=No\n",
      "\n",
      "10\tocéan\tocéan\tPROPN\t_\t_\t7\tnmod\t_\t_\n",
      "\n",
      "11\tfait\tfaire\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\n",
      "\n",
      "12\tl'\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Sing|PronType=Art\t13\tdet\t_\tSpaceAfter=No\n",
      "\n",
      "13\tobjet\tobjet\tNOUN\t_\tGender=Masc|Number=Sing\t11\tobj\t_\t_\n",
      "\n",
      "14\td'\tde\tADP\t_\t_\t16\tcase\t_\tSpaceAfter=No\n",
      "\n",
      "15\tun\tun\tDET\t_\tDefinite=Ind|Gender=Masc|Number=Sing|PronType=Art\t16\tdet\t_\t_\n",
      "\n",
      "16\tinventaire\tinventaire\tNOUN\t_\tGender=Masc|Number=Sing\t13\tnmod\t_\t_\n",
      "\n",
      "17\tZNIEFF\tZNIEFF\tPROPN\t_\t_\t16\tappos\t_\t_\n",
      "\n",
      "18\tde\tde\tADP\t_\t_\t19\tcase\t_\t_\n",
      "\n",
      "19\ttype\ttype\tNOUN\t_\tGender=Masc|Number=Sing\t16\tnmod\t_\t_\n",
      "\n",
      "20\t1\t1\tNUM\t_\t_\t19\tnummod\t_\t_\n",
      "\n",
      "21\ten\ten\tADP\t_\t_\t22\tcase\t_\t_\n",
      "\n",
      "22\t1990\t1990\tNUM\t_\t_\t11\tobl\t_\tSpaceAfter=No\n",
      "\n",
      "23\t.\t.\tPUNCT\t_\t_\t11\tpunct\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code pour ouvrir le fichier 'fr-ud-test.conllu3'\n",
    "# et lire son contenu dans un tableau de strings (un string = les lignes de chaque phrase).\n",
    "# Indiquez le nombre de phrases et affichez la phrase d'indice 400 comme exemple.\n",
    "file = \"fr-ud-test.conllu3\"\n",
    "test_sentences = get_sentences_from_conll(file)\n",
    "\n",
    "print(\"Number of phrases: \", len(test_sentences))\n",
    "print(\"400th phrase: \", test_sentences[399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dans les chaînes obtenues, il reste des mots avec des espaces, que l'on veut supprimer.  \n",
    "# Veuillez écrire ici la ligne de code pour supprimer tous les espaces.\n",
    "test_sentences = list(map(lambda x: x.replace(\" \", \"\"), test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conll_sentence_dg(conll_phrases):\n",
    "    dgs = []\n",
    "    for phrase in test_phrases:\n",
    "        dgs.append(DependencyGraph(phrase, top_relation_label='root'))\n",
    "        \n",
    "    return dgs\n",
    "\n",
    "def get_nsubj_rel(dg):\n",
    "    rels = []\n",
    "    for head, rel, dep in dg.triples():\n",
    "        if rel == 'nsubj':\n",
    "            rels.append((head, rel, dep))\n",
    "    return rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fait, VERB), (pré, NOUN)\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code pour créer un tableau de graphes de dépendances (1 par phrase)\n",
    "# et cherchez les relations de type 'nsubj' dans la phrase d'indice 400 du fichier 'test'.\n",
    "test_dgs = get_conll_sentence_dg(test_sentences)\n",
    "\n",
    "for rel in get_nsubj_rel(test_dgs[399]):\n",
    "    print(\n",
    "        '({h[0]}, {h[1]}), ({d[0]}, {d[1]})'\n",
    "        .format(h=rel[0], d=rel[2])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: pré\n",
      "Verb: fait\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code pour extraire la paire (sujet, verbe)\n",
    "# de la première relation 'nsubj' trouvée ci-dessus.  Il servira ci-dessous.\n",
    "def get_subj_verb_rel(nsubj_rel):\n",
    "    return (nsubj_rel[2][0].lower(), nsubj_rel[0][0].lower())\n",
    "    \n",
    "test_rel = ((\"fait\", \"VERB\"), \"nsubj\", (\"pré\", \"NOUN\"))\n",
    "subject, verb = get_subj_verb_rel(test_rel)\n",
    "\n",
    "print(\"Subject: {}\\nVerb: {}\".format(subject, verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most frequent subject verb pair:\n",
      "[(('il', 'a'), 11), (('on', 'peut'), 5), (('il', 'faut'), 5), ((\"c'\", 'est'), 3), (('il', 'contrôle'), 3), (('il', 'agit'), 3), (('on', 'pourra'), 2), (('on', 'a'), 2), (('qui', 'font'), 2), (('il', 'devient'), 2)]\n"
     ]
    }
   ],
   "source": [
    "# En utilisant les FreqDist, veuillez afficher les 10 paires \n",
    "# (nom, verbe) les plus fréquentes dans le fichier 'test'.\n",
    "# Pensez convertir les mots en minuscule pour consolider le comptage.\n",
    "pairs = []\n",
    "for dg in test_dgs:\n",
    "    pairs = pairs + list(map(get_subj_verb_rel, get_nsubj_rel(dg)))        \n",
    "            \n",
    "fdist = FreqDist(pair for pair in pairs)\n",
    "print(\"Ten most frequent subject verb pair:\")\n",
    "print(fdist.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez consolider ces résultats en regroupant les trois fichiers 'train', 'dev' et 'test'.\n",
    "# Affichez cette fois-ci les 20 paires (sujet, verbe) les plus fréquentes.\n",
    "dev_sentences = get_sentences_from_conll(\"fr-ud-dev.conllu3\")\n",
    "train_sentences = get_sentences_from_conll(\"fr-ud-train.conllu3\")\n",
    "\n",
    "dev_dgs = get_conll_sentence_dg(dev_sentences)\n",
    "train_dgs = get_conll_sentence_dg(train_sentences)\n",
    "\n",
    "dgs = test_dgs + dev_dgs + train_dgs\n",
    "pairs = []\n",
    "for dg in dgs:\n",
    "    pairs = pairs + list(map(get_subj_verb_rel, get_nsubj_rel(dg)))\n",
    "    \n",
    "fdist = FreqDist(pair for pair in pairs)\n",
    "print(\"Twenty most frequent subject verb pair:\")\n",
    "print(fdist.most_common()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tAppliquer un analyseur syntaxique en constituants. Extraire les groupe nominaux\n",
    "\n",
    "Dans cette 3e partie, vous utiliserez l'analyseur syntaxique en constituants appelé **LexicalizedParser**, toujours fourni parmi les outils CoreNLP de Stanford, et [documenté ici](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html).  \n",
    "\n",
    "Le code du parser figure dans le même fichier `.jar` que ci-dessus, et vous pouvez télécharger le modèle `frenchFactored.ser.gz` depuis le [Switch Drive](https://drive.switch.ch/index.php/s/R6gXPifDupAbo3l) de l'enseignant (4 MB).  \n",
    "\n",
    "L'objectif de cette 3e partie est d'extraire les **groupes nominaux** les plus fréquents d'un texte.\n",
    "\n",
    "\n",
    "**Note** : Pour travailler avec les outils CoreNLP depuis NLTK, la solution la plus récente consiste à démarrer un [serveur CoreNLP](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html), soit depuis NLTK soit en ligne de commande.  Pour mémoire, les commandes sont `java -mx4g -cp \"stanford-corenlp-3.9.2.jar;stanford-french-corenlp-2018-01-31-models.jar\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-french.properties -port 9000 -timeout 15000` ou alors `CoreNLPServer(\"stanford-corenlp-3.9.2.jar\", \"stanford-french-corenlp-2018-01-31-models.jar\").start()`. Les [modèles pour le français](http://central.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.9.2/) font 277 MB.  -- Mais nous n'utiliserons pas cela dans ce qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire la ligne de commande (java) qui effecte l'analyse syntaxique\n",
    "# en constituants d'un court texte de votre choix (10 phrases).  Choisissez\n",
    "# 'oneline' comme format des résultats et écrivez-les dans un ficher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous allez utiliser la classe BracketParseCorpusReader de NLTK pour importer les lignes contenant les analyses syntaxiques vers des objets `Tree` sous NLTK, auquels vous pourrez appliquer diverses fonctions (voir https://www.nltk.org/_modules/nltk/tree.html#Tree), comme demandé ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.bracket_parse import BracketParseCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrivez le code qui importe le fichier de résultats du LexicalizedParser comme une liste d'arbres.\n",
    "# Cherchez dans la documentation de Tree une fonction d'affichage joli et affichez le premier arbre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrivez le code qui extrait les groupes nominaux de toutes les phrases (représentés\n",
    "# dans votre liste comme des strings), et qui affiche les 10 les plus fréquents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez enfin répéter l'expérience avec un texte plus long (une page Wikipedia ou un texte du projet Gutenberg) : indiquez les 20 GN les plus fréquents, et le temps approximatif nécessaire pour le parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire 3\n",
    "\n",
    "Merci de nettoyer votre feuille, exécuter une dernière fois toutes les instructions, sauvegarder le résultat, et le soumettre sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
